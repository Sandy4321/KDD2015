{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/home/simengy/Data Science/KDD2015/model'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/simengy/Data Science/KDD2015\n"
     ]
    }
   ],
   "source": [
    "cd /home/simengy/Data\\ Science/KDD2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "enroll_train = pd.read_csv('data/train/enrollment_train.csv', header=False)\n",
    "log_train = pd.read_csv('data/train/log_train.csv', header=False)\n",
    "label_train = pd.read_csv('data/train/truth_train.csv', header=False)\n",
    "\n",
    "course_hirach = pd.read_csv('data/object.csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120542, 3)\n",
      "(8157277, 5)\n",
      "(120541, 2)\n",
      "(27249, 5)\n"
     ]
    }
   ],
   "source": [
    "print enroll_train.shape\n",
    "print log_train.shape\n",
    "print label_train.shape\n",
    "print course_hirach.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrollment_id (120542,)\n",
      "username (79186,)\n",
      "course_id (39,)\n"
     ]
    }
   ],
   "source": [
    "for col in enroll_train.columns:\n",
    "\n",
    "    levels = pd.Categorical(enroll_train[col]).categories\n",
    "    print col, levels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrollment_id (120542,)\n",
      "time (3922167,)\n",
      "source (2,)\n",
      "event (7,)\n",
      "object (5890,)\n"
     ]
    }
   ],
   "source": [
    "for col in log_train.columns:\n",
    "\n",
    "    levels = pd.Categorical(log_train[col]).categories\n",
    "    print col, levels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "course_id (39,)\n",
      "module_id (26750,)\n",
      "category (15,)\n",
      "children (13192,)\n",
      "start (421,)\n"
     ]
    }
   ],
   "source": [
    "for col in course_hirach.columns:\n",
    "\n",
    "    levels = pd.Categorical(course_hirach[col]).categories\n",
    "    #print list(set(log_train['source']))\n",
    "    print col, levels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['about', 'chapter', 'course', 'course_info', 'html', 'outlink',\n",
       "       'problem', 'sequential', 'static_tab', 'vertical', 'video',\n",
       "       'combinedopenended', 'peergrading', 'discussion', 'dictation'], dtype=object)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#course_dict = dict(zip(pd.Categorical(course_hirach['course_id']).unique(),range(39)))\n",
    "course_dict = dict(zip(list(set(course_hirach['category'])),range(39)))\n",
    "pd.Categorical(course_hirach['category']).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "parent\n",
      "['00356MwdHIKtYac8z7ECu16KbvCu7OFx', '005x5aALZuJd7pV5KWd240POzjkEPql7', '00XGdvCJreNAV2Kv8GC5W5vpeFN0CnJU', '00r0Rd6mrrmN1FQgwVd0aBdVD94oPioi', '00sfPTSB0l17kDQP3VkwN8sGE46sc1bR', '00xzYitbaf6rAu6eFY3MNVNIndk24udd', '0105DKstClvZZXfPTEw6oNp1A916gx6w', '010pmQNUh5MGIOyZDT6mCAoktThmpwF5', '01E2qHdCNI4UsHdBqb1N1xUaTX3CzklU', '01INyBxZFK6rEhMin7N1H5IUt6SXgzOB']\n",
      "\n",
      "child\n",
      "['00356MwdHIKtYac8z7ECu16KbvCu7OFx ', '005x5aALZuJd7pV5KWd240POzjkEPql7 TPMDf8jhS2A14HrnlcW5X8Ntfx6UuP2f XgJo6f6FmYYtSpGhAyx0mRIN0hFM20IP L1HoPYrqZVzK9Zi9bM4HlXy0FGHsrTOF VqAG3JhNM5stV4KdgJwcXftORr6ctXH3 ', '00r0Rd6mrrmN1FQgwVd0aBdVD94oPioi ', '00sfPTSB0l17kDQP3VkwN8sGE46sc1bR ', '0105DKstClvZZXfPTEw6oNp1A916gx6w CeufxKzu6lxO63RgZZGlkBtRgNVzNkWd ', '010pmQNUh5MGIOyZDT6mCAoktThmpwF5 ', '01INyBxZFK6rEhMin7N1H5IUt6SXgzOB ', '01Q41c741GB8om41iUyqwpXSptucIgah ', '02ezyDAGd9RV1Mw27BCok6Wko4CKgVhe ', '02oh7ab0ZcuQ6knZjaMppx3dy6LO975e ']\n",
      "\n",
      "log\n",
      "['005x5aALZuJd7pV5KWd240POzjkEPql7', '00r0Rd6mrrmN1FQgwVd0aBdVD94oPioi', '010pmQNUh5MGIOyZDT6mCAoktThmpwF5', '02BCoIpATWDw1hNzU6VWjFsxZtwvKN5X', '02vhdNzPGLlNDQnAysom5ihffeEAUjrT', '03ZTCDvkyYXNeMuLLCTAaWiZR5dhWzZA', '04Tw7xUXD3aVXiPY7Mz5bTGnWtvfQ9Po', '04U2K3iUkwOpoJj09BROXVcZH4uehQnG', '05jmPUYEc2mBYIco2za1ORP5gDoGVtht', '05pzUCqPAVFrkOgHAKcETxkx5IpYQAt2']\n",
      "43 13192\n",
      "170 5890\n"
     ]
    }
   ],
   "source": [
    "# Understand the relation between module_id and child\n",
    "\n",
    "count = 0\n",
    "total = 0\n",
    "\n",
    "log_object_level = pd.Categorical(log_train['object']).unique()\n",
    "child_level = pd.Categorical(course_hirach['children'].dropna()).unique()\n",
    "parent_level = pd.Categorical(course_hirach['module_id']).unique()\n",
    "\n",
    "\n",
    "\n",
    "print '\\nparent\\n', sorted(parent_level)[:10]\n",
    "print '\\nchild\\n', sorted(child_level)[:10]\n",
    "print '\\nlog\\n', sorted(log_object_level)[:10]\n",
    "\n",
    "for key_groups in child_level:\n",
    "    total += 1\n",
    "    for key in key_groups.split():\n",
    "        if key not in parent_level:\n",
    "            count += 1\n",
    "        #raise ValueError(key, 'not here')\n",
    "print count, total\n",
    "\n",
    "\n",
    "count = 0\n",
    "total = 0\n",
    "\n",
    "for key in log_object_level:\n",
    "    total += 1\n",
    "    \n",
    "    if key not in parent_level:\n",
    "        count += 1\n",
    "    \n",
    "print count, total        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "26750\n",
      "13193\n",
      "['nagivate' 'access' 'problem' 'page_close' 'video' 'discussion' 'wiki']\n"
     ]
    }
   ],
   "source": [
    "print len(pd.Categorical(course_hirach['category']).unique())\n",
    "print len(pd.Categorical(course_hirach['module_id']).unique())\n",
    "print len(pd.Categorical(course_hirach['children']).unique())\n",
    "print pd.Categorical(log_train['event']).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'enrollment_id', u'username', u'course_id', u'time', u'source', u'event', u'object'], dtype='object')\n",
      "(10000, 7)\n",
      "0    2014-06-14T09:38:29\n",
      "1    2014-06-14T09:38:39\n",
      "2    2014-06-14T09:38:39\n",
      "3    2014-06-14T09:38:48\n",
      "4    2014-06-14T09:41:49\n",
      "5    2014-06-14T09:41:50\n",
      "6    2014-06-14T09:42:28\n",
      "7    2014-06-14T09:42:30\n",
      "8    2014-06-14T09:43:20\n",
      "9    2014-06-14T09:43:25\n",
      "Name: time, dtype: object\n",
      "0                   null\n",
      "1                   null\n",
      "2    2014-08-11T01:00:00\n",
      "3    2014-07-28T01:00:00\n",
      "4    2014-08-25T01:00:00\n",
      "5    2014-09-15T01:00:00\n",
      "6    2014-10-16T01:00:00\n",
      "7    2014-09-08T01:00:00\n",
      "8    2014-06-02T01:00:00\n",
      "9    2014-08-04T01:00:00\n",
      "Name: start, dtype: object\n"
     ]
    }
   ],
   "source": [
    "merged1 = pd.merge(enroll_train[:10000], log_train[:10000], on=['enrollment_id', 'enrollment_id'] )\n",
    "print merged1.columns\n",
    "print merged1.shape\n",
    "\n",
    "dummy = pd.get_dummies(merged1['course_id'])\n",
    "print merged1['time'][:10]\n",
    "print course_hirach['start'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79293352469284306"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train['0'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "for col in enroll_train.columns:\n",
    "\n",
    "    levels = pd.Categorical(enroll_train[col]).levels\n",
    "    print col, levels.shape\n",
    "\n",
    "    \n",
    "print \n",
    "for col in log_train.columns:\n",
    "\n",
    "    levels = pd.Categorical(log_train[col]).levels\n",
    "    print col, levels.shape\n",
    "    \n",
    "    if col == 'event':\n",
    "        print enc.fit_transform(log_train[col].T.to_dict().values())    \n",
    "    if col == 'object':\n",
    "        print enc.fit_transform(log_train[col].T.to_dict().values())\n",
    "        \n",
    "        \n",
    "        \n",
    "    if col == 'sources':\n",
    "        \n",
    "        pd.core.reshape.get_dummies(log_train[col])\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The script MUST contain a function named azureml_main\n",
    "# which is the entry point for this module.\n",
    "#\n",
    "# The entry point function can contain up to two input arguments:\n",
    "#   Param<dataframe1>: a pandas.DataFrame\n",
    "#   Param<dataframe2>: a pandas.DataFrame\n",
    "\n",
    "# This module extracts 30 features from the log event data, including weekly trending slope, number of sessions, \n",
    "# duration of sessions, average number of events in each session, etc. More details, please read the comments before\n",
    "# function extract_enrollment_log()\n",
    "from numpy import *\n",
    "import datetime, time\n",
    "import pandas as pd\n",
    "\n",
    "#This function extracts statistics for each session in an enrollment, such as average session duration, standard deviation\n",
    "# of session duration, and min and max session duration. All in unit hours.\n",
    "# Inputs: data is an array with 5 columns,  \n",
    "# Col1: year, \n",
    "# Col2: week number, \n",
    "# Col3: session id when new session starts if two events are apart for more than 3 hours, \n",
    "# Col4: session id when new session starts if two events are apart for more than 1 hour, \n",
    "# Col5: time stamp of the event\n",
    "# Note: session id starts from 1\n",
    "# The second input is the column index of the session id. When calculating the statistics of 3-hour defined sessions, \n",
    "# session id index= 2, when calculating the statistics of 1-hour defined sessions, session id index=3, \n",
    "# Outputs: session_stat, a list of 4 float values, in the unit of hours. \n",
    "# Value1: average duration of sessions\n",
    "# Value2: if session number>1, standard deviation of session duration. Otherwise, 0\n",
    "# Value3: maximal session duration\n",
    "# Value4: minimal session duration\n",
    "def extract_session_stat(data,sessionid_index):\n",
    "    numrows = data.shape[0]\n",
    "    sessionindex = 0\n",
    "    numsessions = max(data[:,sessionid_index])\n",
    "    session_min = [float(inf)]*numsessions\n",
    "    session_max = [0]*numsessions\n",
    "    session_stat = [0]*4\n",
    "    for i in range(numrows):\n",
    "        sessionindex = int(data[i,sessionid_index])-1\n",
    "        if session_min[sessionindex] > data[i,4]:\n",
    "            session_min[sessionindex] = data[i,4]\n",
    "        if session_max[sessionindex] < data[i,4]:\n",
    "            session_max[sessionindex] = data[i,4]\n",
    "    session_duration = (array(session_max)-array(session_min))/3600\n",
    "    \n",
    "    session_stat[0] = mean(session_duration)\n",
    "    if numsessions > 1:\n",
    "        session_stat[1] = std(session_duration)\n",
    "    session_stat[2] = max(session_duration)\n",
    "    session_stat[3] = min(session_duration)\n",
    "    return session_stat\n",
    "\n",
    "# This function extracts statistics for each enrollment, and call function extract_session_stat to extract statistics for sessions \n",
    "# Input:  an array with two columns:\n",
    "# Col1: enrollment id\n",
    "# Col2: timestamps of log events, in ascending order\n",
    "# Output:  A list of 31 log event statistics, These are the output features of this Python module\n",
    "# Value1: enrollment_id\n",
    "# Value2: trending slope of the weekly number of events within the enrollment\n",
    "# Value3: number of events in the last week of the enrollment\n",
    "# Value4: number of events in the first week of the enrollment\n",
    "# Value5: number of events in the week before the last week of the enrollment\n",
    "# Value6: average weekly number of events in the enrollment period\n",
    "# Value7: standard deviation of the weekly number of events in the enrollment period\n",
    "# Value8: maximal weekly number of events in the enrollment period\n",
    "# Value9: minimal weekly number of events in the enrollment period\n",
    "# Value10: month (1-12) of the first event in the enrollment\n",
    "# Value11: month of the last event in the enrollment\n",
    "# Value12: number of 3-hour defined sessions in the enrollment\n",
    "# Value13: average number of events in 3-hour defined sessions in the enrollment\n",
    "# Value14: standard deviation of the number of events in 3-hour defined sessions in the enrollment\n",
    "# Value15: maximal number of events in 3-hour defined sessions in the enrollment\n",
    "# Value16: minimal number of events in 3-hour defined sessions in the enrollment\n",
    "# Values 17 & 18: coefficients b and c in the polynomial model y = a + bx + cx**2, where x is week number (all start from 0), and \n",
    "# y is the weekly number of events\n",
    "# Value19: number of 1-hour defined sessions in the enrollment\n",
    "# Value20: average number of events in 1-hour defined sessions in the enrollment\n",
    "# Value21: standard deviation of the number of events in 1-hour defined sessions in the enrollment\n",
    "# Value22: maximal number of events in 1-hour defined sessions in the enrollment\n",
    "# Value23: minimal number of events in 1-hour defined sessions in the enrollment\n",
    "# Values 24-27: statistics of 3-hour defined sessions\n",
    "# Values28-31: statistics of 1-hour defined sessions\n",
    "def extract_enrollment_log(log):\n",
    "    num_events = len(log)\n",
    "    chunk_data = zeros((num_events,5)) \n",
    "    features = [0]*31\n",
    "    features[0] = log[0][0]\n",
    "    previous_stamp_3hr = 0\n",
    "    previous_stamp_1hr = 0\n",
    "    session_id_3hr = 0\n",
    "    session_id_1hr = 0\n",
    "    min_year = 3000\n",
    "    max_year = 1000\n",
    "    max_week = 0\n",
    "# min_timestamp and max_timestamp will be used to calculate the duration of \n",
    "# the enrollment\n",
    "    min_timestamp = float('inf')\n",
    "    max_timestamp = 0\n",
    "    for i in range(num_events):\n",
    "        timestamp_i = log[i][1]\n",
    "        if min_timestamp > timestamp_i:\n",
    "            min_timestamp = timestamp_i\n",
    "        if max_timestamp < timestamp_i:\n",
    "            max_timestamp = timestamp_i\n",
    "            \n",
    "# Assign session id to a record. If two consecutive records are apart for over 3 hours for longer session definition\n",
    "# (or 1 hour for short session definition), a new session id starts\n",
    "        if timestamp_i - previous_stamp_3hr >= 10800: \n",
    "            session_id_3hr += 1\n",
    "        if timestamp_i - previous_stamp_1hr >= 3600:\n",
    "            session_id_1hr += 1\n",
    "        previous_stamp_3hr = timestamp_i\n",
    "        previous_stamp_1hr = timestamp_i\n",
    "        chunk_data[i,4] = timestamp_i\n",
    "        chunk_data[i,2] = session_id_3hr\n",
    "        chunk_data[i,3] = session_id_1hr\n",
    "        date_object = datetime.datetime.fromtimestamp(timestamp_i)\n",
    "        # The year and week number of each record\n",
    "# These two variables will be used to calculate the trending of events in \n",
    "# each week.\n",
    "        year_num = date_object.year\n",
    "        week_num = int(date_object.strftime(\"%U\"))\n",
    "        if year_num > max_year:\n",
    "            max_year = year_num\n",
    "        if year_num < min_year:\n",
    "            min_year = year_num\n",
    "        if week_num > max_week:\n",
    "            max_week = week_num\n",
    "        chunk_data[i,0] = year_num\n",
    "        chunk_data[i,1] = week_num\n",
    "# if the enrollment is in two years (crossing the new year's day)\n",
    "# we need to reassign the week number in the second year\n",
    "# to make sure that the week number is ascending with dates\n",
    "    if max_year > min_year:\n",
    "        week_last_day = datetime.date(min_year, 12, 31).isocalendar()[1]\n",
    "        if week_last_day == 1:\n",
    "            week_last_day = datetime.date(min_year, 12, 24).isocalendar()[1]\n",
    "        max_year_index = [i for i, j in enumerate(chunk_data[:,0]) if j == max_year]\n",
    "        chunk_data[max_year_index,1] += week_last_day\n",
    "        min_year_index = [i for i, j in enumerate(chunk_data[:,0]) if j == min_year]\n",
    "        week1_index = [i for i, j in enumerate(chunk_data[:,1]) if j == 1]\n",
    "        intersection = list(set(min_year_index).intersection(week1_index))\n",
    "        if len(intersection)>0:\n",
    "            chunk_data[intersection,1] += week_last_day\n",
    "        max_week = max(chunk_data[:,1])\n",
    "    min_week = min(chunk_data[:,1])\n",
    "    num_weeks = int(max_week - min_week)\n",
    "    events_count = [0]*(num_weeks+1)\n",
    "    num_sessions_3hr = max(chunk_data[:,2])\n",
    "    num_sessions_1hr = max(chunk_data[:,3])\n",
    "    session_count_3hr = [0]*num_sessions_3hr\n",
    "    session_count_1hr = [0]*num_sessions_1hr\n",
    "    for i in range(num_events):\n",
    "        events_count[int(chunk_data[i,1]-min_week)] += 1\n",
    "        session_count_3hr[int(chunk_data[i,2]-1)] += 1\n",
    "        session_count_1hr[int(chunk_data[i,3]-1)] += 1\n",
    "    if num_weeks > 0:\n",
    "        weeks = range(num_weeks+1)\n",
    "        events_count = array(events_count)\n",
    "        A = vstack([weeks, ones(len(weeks))]).T\n",
    "        m, c = linalg.lstsq(A, events_count)[0]\n",
    "        features[1] = m\n",
    "    if num_weeks > 1:\n",
    "        weeks = range(num_weeks+1) \n",
    "        events_count = array(events_count)\n",
    "        #fit a polynomial model y = a + bx + cx**2\n",
    "        z = polyfit(weeks, events_count, 2) \n",
    "        features[16] = z[1]\n",
    "        features[17] = z[2]\n",
    "    features[2] = events_count[-1]\n",
    "    features[3] = events_count[0]\n",
    "    if num_weeks > 0:\n",
    "        features[4] = events_count[-2]\n",
    "    features[5] = mean(events_count)\n",
    "    if num_weeks > 0:\n",
    "        features[6] = std(events_count)\n",
    "    features[7] = max(events_count)\n",
    "    features[8] = min(events_count)\n",
    "    date_object = datetime.datetime.fromtimestamp(min_timestamp)\n",
    "    features[9] = date_object.month\n",
    "    date_object = datetime.datetime.fromtimestamp(max_timestamp)\n",
    "    features[10] = date_object.month\n",
    "    features[11] = num_sessions_3hr\n",
    "    features[12] = mean(session_count_3hr)\n",
    "    if num_sessions_3hr > 1:\n",
    "        features[13] = std(session_count_3hr)\n",
    "    features[14] = max(session_count_3hr)\n",
    "    features[15] = min(session_count_3hr)\n",
    "    features[18] = num_sessions_1hr\n",
    "    features[19] = mean(session_count_1hr)\n",
    "    if num_sessions_1hr > 1:\n",
    "        features[20] = std(session_count_1hr)\n",
    "    features[21] = max(session_count_1hr)\n",
    "    features[22] = min(session_count_1hr)\n",
    "    session_stat_3hr = extract_session_stat(chunk_data,2)\n",
    "    session_stat_1hr = extract_session_stat(chunk_data,3)\n",
    "    features[23:27] = session_stat_3hr\n",
    "    features[27:31] = session_stat_1hr\n",
    "    return features\n",
    "\n",
    "# azureml_main function is the main function that is called during execution. \n",
    "def azureml_main_enrollment(dataframe1 = None, dataframe2 = None):\n",
    "    \n",
    "    num_obs = dataframe1.shape[0]\n",
    "    num_enrollment = len(set(dataframe1['enrollment_id']))\n",
    "    print(\"There are %d unique enrollment ids.\"%num_enrollment)\n",
    "    # The output will be a data frame which has num_enrollment rows, and 31 columns, where the first column is the enrollment id,\n",
    "    # and the remaining 30 columns are features\n",
    "    output_df = zeros((num_enrollment,31))\n",
    "    enrollment_log = []\n",
    "    previous_id = -1\n",
    "    enrollment_index = 0\n",
    "    #Partition log records by enrollment_id, and then extract statistics for each \n",
    "#enrollment\n",
    "    for i in range(num_obs):\n",
    "      current_id = dataframe1.iloc[i][0]\n",
    "      # If a new enrollment_id starts, indicating that we have collected all log records\n",
    "# for that enrollment_id, then we can start processing log records of the enrollment_id\n",
    "      if current_id != previous_id:\n",
    "          if i > 0:\n",
    "              output_df[enrollment_index,:] = extract_enrollment_log(enrollment_log)\n",
    "              enrollment_index += 1\n",
    "          enrollment_log = []\n",
    "          previous_id = current_id\n",
    "      enrollment_log.append(dataframe1.iloc[i])\n",
    "    output_df[enrollment_index,:] = extract_enrollment_log(enrollment_log)\n",
    "# The output of the Execute Python Script has to be a Pandas data frame. Column names are added to the data frame\n",
    "# for the convenience of referring to columns in the consequential modules.\n",
    "    dataframe1 =  pd.DataFrame(output_df) \n",
    "    dataframe1.columns = ['enrollment_id','event_trend','events_last_week',\\\n",
    "                          'events_first_week','events_second_last_week',\\\n",
    "                          'weekly_avg','weekly_std','max_weekly_count','min_weekly_count',\\\n",
    "                          'first_event_month','last_event_month','session_count_3hr',\\\n",
    "                          'session_avg_3hr','session_std_3hr','session_max_3hr','session_min_3hr',\\\n",
    "                          'quadratic_b','quadratic_c','session_count_1hr',\\\n",
    "                          'session_avg_1hr','sessioin_std_1hr','sessioin_max_1hr',\\\n",
    "                          'session_min_1hr','session_dur_avg_3hr','session_dur_std_3hr',\\\n",
    "                          'sessioin_dur_max_3hr','session_dur_min_3hr','sessioin_dur_avg_1hr',\\\n",
    "                          'session_dur_std_1hr','session_dur_max_1hr','session_dur_min_1hr']\n",
    "    return dataframe1\n",
    "    # If a zip file is connected to the third input port is connected,\n",
    "    # it is unzipped under \".\\Script Bundle\". This directory is added\n",
    "    # to sys.path. Therefore, if your zip file contains a Python file\n",
    "    # mymodule.py you can import it using:\n",
    "    # import mymodule\n",
    "    \n",
    "    # Return value must be of a sequence of pandas.DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The script MUST contain a function named azureml_main\n",
    "# which is the entry point for this module.\n",
    "#\n",
    "# The entry point function can contain up to two input arguments:\n",
    "#   Param<dataframe1>: a pandas.DataFrame\n",
    "#   Param<dataframe2>: a pandas.DataFrame\n",
    "# This module develop the counting features of log events.\n",
    "# The output is a dataframe with 41 columns:\n",
    "# Col1: enrollment id\n",
    "# Cols 2-8: counts of events in Monday to Sunday\n",
    "# Cols 9-32: counts of events in hour 0-23\n",
    "# Cols 33-39: counts of event types\n",
    "# Cols 40-41: counts of source types\n",
    "# Cols 42-80: counts of course types\n",
    "from numpy import *\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def azureml_main_count(dataframe1 = None, dataframe2 = None):\n",
    "\n",
    "    # Execution logic goes here\n",
    "    unique_enrollment = list(set(dataframe1.iloc[:,0]))\n",
    "# Define dictionaries to map enrollment_id to row indices\n",
    "    num_unique_enrollment = len(unique_enrollment)\n",
    "    enrollment_dict = dict(zip(unique_enrollment,range(num_unique_enrollment)))\n",
    "    numrows = dataframe1.shape[0]\n",
    "    count_features = zeros((num_unique_enrollment, 80))\n",
    "    # define dictionaries to map source names and event types to indices\n",
    "    source_dict = {'browser':0,'server':1}\n",
    "    event_dict = dict(zip(['access','problem','page_close',\\\n",
    "        'nagivate','video','discussion','wiki'],range(7)))\n",
    "    course_dict = dict(zip(pd.Categorical(datarame1['course_id']).unique(),range(39)))\n",
    "         \n",
    "    for i in range(numrows):\n",
    "        enrollment_id = dataframe1.iloc[i,0]\n",
    "        row_index = enrollment_dict[enrollment_id]\n",
    "        count_features[row_index,0] = enrollment_id\n",
    "        timestamp_i = float(dataframe1.iloc[i,1])\n",
    "        dateobj = datetime.fromtimestamp(timestamp_i)\n",
    "        weekday = dateobj.weekday()\n",
    "        hour = dateobj.hour\n",
    "        #weekday is between 0 and 6, where Monday is 0, and Sunday is 6\n",
    "        count_features[row_index,weekday+1] += 1  \n",
    "        # hour is between 0 and 23\n",
    "        count_features[row_index,hour+8] += 1\n",
    "        event_index = event_dict[dataframe1.iloc[i,3]]\n",
    "        source_index = source_dict[dataframe1.iloc[i,2]]\n",
    "        count_features[row_index,event_index+32] += 1\n",
    "        count_features[row_index,source_index+39] += 1\n",
    "        # course is between 0-38\n",
    "        count_features[row_index,source_index+41] += 1\n",
    "        \n",
    "    dataframe1 = pd.DataFrame(count_features)\n",
    "    \n",
    "    dataframe1.columns = ['enrollment_id','MonCount',\\\n",
    "        'TueCount','WedCount','ThuCount','FriCount',\\\n",
    "        'SatCount','SunCount','Hr0Count','Hr1Count','Hr2Count',\\\n",
    "        'Hr3Count','Hr4Count','Hr5Count','Hr6Count',\\\n",
    "        'Hr7Count','Hr8Count','Hr9Count','Hr10Count',\\\n",
    "        'Hr11Count','Hr12Count','Hr13Count','Hr14Count',\\\n",
    "        'Hr15Count','Hr16Count','Hr17Count','Hr18Count',\\\n",
    "        'Hr19Count','Hr20Count','Hr21Count','Hr22Count',\\\n",
    "        'Hr23Count','AccCount','ProCount','PagCount',\\\n",
    "        'NagCount','VidCount','DisCount','WikCount',\\\n",
    "        'BroCount','SerCount', \\\n",
    "        'Course0Count', 'Course1Count', 'Course2Count', 'Course3Count', 'Course4Count', 'Course5Count', \\\n",
    "        'Course6Count', 'Course7Count', 'Course8Count', 'Course9Count', 'Course10Count', 'Course11Count', \\\n",
    "        'Course12Count', 'Course13Count', 'Course14Count', 'Course15Count', 'Course16Count', 'Course17Count', \\\n",
    "        'Course18Count', 'Course19Count', 'Course20Count', 'Course21Count', 'Course22Count', 'Course23Count', \\\n",
    "        'Course24Count', 'Course25Count', 'Course26Count', 'Course27Count', 'Course28Count', 'Course29Count', \\\n",
    "        'Course30Count', 'Course31Count', 'Course32Count', 'Course33Count', 'Course34Count', 'Course35Count', \\\n",
    "        'Course36Count', 'Course37Count', 'Course38Count']\n",
    "    # If a zip file is connected to the third input port is connected,\n",
    "    # it is unzipped under \".\\Script Bundle\". This directory is added\n",
    "    # to sys.path. Therefore, if your zip file contains a Python file\n",
    "    # mymodule.py you can import it using:\n",
    "    # import mymodule\n",
    "    \n",
    "    # Return value must be of a sequence of pandas.DataFrame\n",
    "    return dataframe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Course0Count', 'Course1Count', 'Course2Count', 'Course3Count', 'Course4Count', 'Course5Count', 'Course6Count', 'Course7Count', 'Course8Count', 'Course9Count', 'Course10Count', 'Course11Count', 'Course12Count', 'Course13Count', 'Course14Count', 'Course15Count', 'Course16Count', 'Course17Count', 'Course18Count', 'Course19Count', 'Course20Count', 'Course21Count', 'Course22Count', 'Course23Count', 'Course24Count', 'Course25Count', 'Course26Count', 'Course27Count', 'Course28Count', 'Course29Count', 'Course30Count', 'Course31Count', 'Course32Count', 'Course33Count', 'Course34Count', 'Course35Count', 'Course36Count', 'Course37Count', 'Course38Count']\n",
      "['browser', 'server']\n"
     ]
    }
   ],
   "source": [
    "course = []\n",
    "\n",
    "for i in xrange(0,39):\n",
    "    course.append('Course'+str(i)+'Count')\n",
    "\n",
    "print course\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 120542 unique enrollment ids.\n",
      "It takes time = 0:27:06.329222\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "log_train['time'] = (pd.to_datetime(log_train['time']) - datetime.datetime(2012,1,1)) / np.timedelta64(1, 's')\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "new_features_1 = azureml_main_enrollment(dataframe1 = log_train, dataframe2 = None)\n",
    "\n",
    "print 'It takes time =', datetime.datetime.now() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_features_1.to_csv('model/feature/enrollment_feature.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27249,), (2324,))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_hirach['start'].shape, course_hirach[course_hirach['start']!='null']['start'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/simengy/git/xgboost/wrapper/')\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "y_columns = [name for name in train_with_labels.columns if name.startswith('y')]\n",
    "\n",
    "X_numerical_base, X_numerical_meta, X_sparse_base, X_sparse_meta, y_base, y_meta = train_test_split(\n",
    "    X_numerical, \n",
    "    X_sparse, \n",
    "    train_with_labels[y_columns].values,\n",
    "    test_size = 0.5\n",
    ")\n",
    "\n",
    "X_meta = [] \n",
    "X_test_meta = []\n",
    "\n",
    "print \"Build meta\"\n",
    "\n",
    "\n",
    "param = {'bst:max_depth':8, 'bst:eta':0.2, 'silent':0, 'objective':'binary:logistic' }\n",
    "        param['nthread'] = 16\n",
    "        plst = param.items()\n",
    "        #plst += [('eval_metric', 'auc')] # Multiple evals can be handled in this way\n",
    "        plst += [('eval_metric', 'logloss')]\n",
    "        num_round = 100\n",
    "        \n",
    "        dX_base = xgb.DMatrix(X_numerical_base, label = y)\n",
    "        bst = xgb.train(plst, dX_base, num_round)\n",
    "        \n",
    "        dX_num_meta = xgb.DMatrix(X_numerical_meta)\n",
    "        X_meta.append(bst.predict(dX_num_meta))\n",
    "        \n",
    "        dX_test_num = xgb.DMatrix(X_test_numerical)\n",
    "        X_test_meta.append(bst.predict(dX_test_num))\n",
    "        \n",
    "        print i, 'xgboost = ', datetime.now() - t1\n",
    "        t2 = datetime.now()\n",
    "        \n",
    "        logit = LogisticRegression(C=0.01, tol=0.000001)\n",
    "        logit.fit(X_sparse_base, y)\n",
    "        X_meta.append(logit.predict_proba(X_sparse_meta))\n",
    "        X_test_meta.append(logit.predict_proba(X_test_sparse))\n",
    "        \n",
    "        print i, 'logit = ', datetime.now() - t2\n",
    "        \n",
    "X_meta = np.column_stack(X_meta)\n",
    "X_test_meta = np.column_stack(X_test_meta)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
